{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "class Sigmoid():\n",
    "    def __call__(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def gradient(self, x):\n",
    "        return self.__call__(x) * (1 - self.__call__(x))\n",
    "\n",
    "class Softmax():\n",
    "    def __call__(self, x):\n",
    "        e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "        return e_x / np.sum(e_x, axis=-1, keepdims=True)\n",
    "\n",
    "    def gradient(self, x):\n",
    "        p = self.__call__(x)\n",
    "        return p * (1 - p)\n",
    "    \n",
    "class MLP():\n",
    "    \"\"\"Multilayer Perceptron classifier. A fully-connected neural network with one hidden layer.\n",
    "    Unrolled to display the whole forward and backward pass.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    nHidden: int:\n",
    "        The number of processing nodes (neurons) in the hidden layer. \n",
    "    epoch: float\n",
    "        The number of training iterations the algorithm will tune the weights for.\n",
    "    eta: float\n",
    "        The step length that will be used when updating the weights.\n",
    "    \"\"\"\n",
    "    def __init__(self, nHidden, epoch=3000, eta=0.01):\n",
    "        self.nHidden = nHidden\n",
    "        self.epoch = epoch\n",
    "        self.eta = eta\n",
    "        self.hFunc = Sigmoid()\n",
    "        self.oFunc = Softmax()\n",
    "\n",
    "    def _initialize_weights(self, X, y):\n",
    "        n_features = X.shape[1]\n",
    "        n_outputs = y.shape[0]\n",
    "        # Hidden layer\n",
    "        limit   = 1 / math.sqrt(n_features)\n",
    "        self.W  = np.random.uniform(-limit, limit, (n_features, self.nHidden))\n",
    "        self.w0 = np.zeros(self.nHidden)\n",
    "        # Output layer\n",
    "        limit   = 1 / math.sqrt(self.nHidden)\n",
    "        self.V  = np.random.uniform(-limit, limit, (self.nHidden, n_outputs))\n",
    "        self.v0 = np.zeros(n_outputs)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "\n",
    "        self._initialize_weights(X, y)\n",
    "\n",
    "        for ep in range(self.epoch):\n",
    "            for t in range(X.shape[0]):\n",
    "                # ..............\n",
    "                #  Forward Pass\n",
    "                # ..............\n",
    "\n",
    "                # HIDDEN LAYER\n",
    "                hidden_input = X[t].dot(self.W) - self.w0\n",
    "                hidden_output = self.hFunc(hidden_input)\n",
    "                # OUTPUT LAYER\n",
    "                output_layer_input = hidden_output.dot(self.V) - self.v0\n",
    "                y_pred = self.oFunc(output_layer_input)\n",
    "\n",
    "                # ...............\n",
    "                #  Backward Pass\n",
    "                # ...............\n",
    "\n",
    "                # OUTPUT LAYER\n",
    "                deltaO = (y[t]-y_pred)*(self.oFunc.gradient(output_layer_input))\n",
    "\n",
    "                # HIDDEN LAYER\n",
    "                deltaH = np.zeros(len(hidden_output))\n",
    "                for i in range(len(hidden_input)):\n",
    "                    deltaH[i] = (output_layer_input[i].dot(deltaO))*(self.hFunc.gradient(hidden_output))\n",
    "\n",
    "                # Update weights (by gradient descent)\n",
    "                # Move against the gradient to minimize loss\n",
    "                for i in range(len(self.V)):\n",
    "                    for j in range(len(self.V[0])):\n",
    "                        self.V[i][j] += self.eta*deltaO[j]*output_layer_input[i]\n",
    "                for i in range(len(self.V0)):\n",
    "                    self.V0 += self.eta * deltaO[j] * (-1)\n",
    "\n",
    "                for i in range(len(self.W)):\n",
    "                    for j in range(len(self.W[0])):\n",
    "                        self.W[i][j] += self.eta*deltaH[j]*output_layer_input[i]\n",
    "                for i in range(len(self.W0)):\n",
    "                    self.W0 += self.eta * deltaH[j] * (-1)\n",
    "\n",
    "    # Use the trained model to predict labels of X\n",
    "    def predict(self, X):\n",
    "        # Forward pass:\n",
    "        hidden_input = X.dot(self.W) + self.w0\n",
    "        hidden_output = self.hFunc(hidden_input)\n",
    "        output_layer_input = hidden_output.dot(self.V) + self.v0\n",
    "        y_pred = self.oFunc(output_layer_input)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# fig = plt.figure\n",
    "# for i in range(9):\n",
    "#     plt.imshow(train_images[i], cmap='gray_r')\n",
    "#     plt.show()\n",
    "\n",
    "y = np.zeros((train_labels.shape[0],10))\n",
    "for idx in range(train_labels.shape[0]):\n",
    "    y[idx][train_labels[idx]] = 1\n",
    "    \n",
    "\n",
    "model = MLP(5)\n",
    "model.fit(train_images.reshape(train_images.shape[0],784),y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
